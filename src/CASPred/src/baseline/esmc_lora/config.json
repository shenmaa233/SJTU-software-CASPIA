{
  "batch_size": 64,
  "num_epochs": 100,
  "learning_rate": 0.0001,
  "patience": 10,
  "warmup_epochs": 5,
  "lora_rank": 8,
  "lora_alpha": 16,
  "lora_dropout": 0.1
}